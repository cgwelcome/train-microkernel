## Kernel (Part 1)
Features from the kernel:
  - Context switch
  - Task Scheduler
  - Taks System Call: Create, Yield, Exit, MyTid, MyParentTid

### Folder Structure

| File | Description |
| ------ | ------ |
| kernel.c | Kernel entry point, Syscall handler, First user task initialization |
| application/k1.c | User code |
| kern/tasks.c | Task scheduler, Context Switch, Task methods  |
| user/tasks.c | ABI: User-side system call specification |


### Algorithms

The following code snippet is the core structure of our kernel
```c
void kernel_entry() {
    initialize(); 
    for (;;) {
        unsigned int nextTID = task_schedule();
        if (nextTID == -1) break;
        int request = task_activate(nextTID);
        syscall_handle(nextTID, request);
    }   
}
```
- *initialize* initialize and creates the first user task
- *task_schedule* choose the next task schedule to be schedule
- *task_activate* context switch to the chosen task, add return code if necessary, and when the chosen task invokes system call, it return its parameters
- *system_handle* translates system call to a task decriptor method, and saves the return value to the task that called system call

The scheduler uses the Completely Fair Scheduler Algorithm (CFS). The idea is that every task gets a percentage % of the total CPU runtime which is determined by the priority. The higher priority it is, the higher % of the total CPU runtime it gets. Every task keeps track of the runtime it has used. The next task to be scheduled is the task with the minimum value after apply the following formula. The calibration is there to avoid 0 for new tasks
```c
((task_runtime * total_priority)+SCHEDULER_CALIBRATION)/ priority
```
This implementation of the scheduler the task can give any value to the task priority, and gives a fair share of the algorithm to every task.

### Data Stucture

Simply used an fixed sized array to keep track the number of task descriptor. The maximum number of task descriptors is 128.
Kernel stack get 1 MB allocate. Each task get 128 KB of stack space.

### Output Rationale

The follow is the output of our program
```
Created: 1
Created: 2
TID: 3, PTID: 0
TID: 3, PTID: 0
Created: 3
TID: 4, PTID: 0
TID: 4, PTID: 0
Created: 4
TID: 1, PTID: 0
TID: 2, PTID: 0
TID: 1, PTID: 0
TID: 2, PTID: 0
```
- The first user task has TID of 0
- When Task 1, and Task 2, since they are lower priority, they will be scheduled after the first user task
- When Task 3, and Task 4, the kernel will execute those task over the first user task
- After the first user task has exited, the task 1 and 2, since they have the same priority, same share of CPU, it will toggle when called Yield
