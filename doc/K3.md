# Kernel (Part 3)
Features added to the kernel for this part:
  - Hardware interrupt (AwaitEvent)
  - Idle Task
  - Clock Server (Time, Delay, Delay)
  - Further Optimize Kernel performance while context switching

## Folder Structure

| File | Description |
| ------ | ------ |
| kern/irq.c | AwaitEvent and Interrupt Request interface |
| user/irq.c | User interface for AwaitEvent  |
| server/clock.c | Clock Server and Notifier  |
| user/clock.c | User interface for Clock Server |
| server/idle.c | Idle task  |
| application/k3.c | Kernel 3 test |
| utils/syscon.c | System Controller interface |
| kern/switchframe.S | SWI and Hardware interrupt entry point |

## Algorithms and Data 

### Hardware Interrupt

There are now two entry points from user to kernel. One for software interrupt, and hardware interrupt. Even though there two entries, they share the same return address, and the same `systemcall_handler` to order to avoid any more duplicate code. We have decided to store all the registers value on the user task stack like the way we did when calling `swi`. Therefore, the exit of kernel is the same whether it has been entered as hardware or software interrupt.

For each eventid, we will have a queue implemented as ring buffer. When a task call await event on the an eventid, it will queue itself up that eventid queue. After the hardware gets an interrupt, it will wake up all the tasks waiting on that interrupt. This data structure gives us the simple ability to queue up tasks, and pop every tasks that is waiting on that task

### Clock server

For our clock server, we have use a priority queue data structure implemented as a min heap. Since some clients need to be notified after a certain delay request, and each client have a different delay request, we keep track of the target number of ticks that will trigger a wakeup call to a task. For every tick update, we check if we need to wake up a task. By storing in heap, we simply, need to check the minimum element.

The clock server will create a clock notifier that calls `AwaitEvent` on timer interrupts. When the notifier gets unblocked, it will make a update request to the clock server.

## Output Rationale
```
CPU Usage: ---
TID: 5 - Ticks: 10 - Num: 1
TID: 5 - Ticks: 10 - Num: 2
TID: 6 - Ticks: 23 - Num: 1
TID: 5 - Ticks: 10 - Num: 3
TID: 7 - Ticks: 33 - Num: 1
TID: 6 - Ticks: 23 - Num: 2
TID: 5 - Ticks: 10 - Num: 5
TID: 5 - Ticks: 10 - Num: 6
TID: 7 - Ticks: 33 - Num: 2
TID: 6 - Ticks: 23 - Num: 3
TID: 5 - Ticks: 10 - Num: 7
TID: 8 - Ticks: 71 - Num: 1
TID: 5 - Ticks: 10 - Num: 8
TID: 5 - Ticks: 10 - Num: 9
TID: 6 - Ticks: 23 - Num: 4
TID: 7 - Ticks: 33 - Num: 3
TID: 5 - Ticks: 10 - Num: 10
TID: 5 - Ticks: 10 - Num: 11
TID: 6 - Ticks: 23 - Num: 5
TID: 5 - Ticks: 10 - Num: 12
TID: 5 - Ticks: 10 - Num: 13
TID: 7 - Ticks: 33 - Num: 4
TID: 6 - Ticks: 23 - Num: 6
TID: 5 - Ticks: 10 - Num: 14
TID: 8 - Ticks: 71 - Num: 2
TID: 5 - Ticks: 10 - Num: 15
TID: 5 - Ticks: 10 - Num: 16
TID: 6 - Ticks: 23 - Num: 7
TID: 7 - Ticks: 33 - Num: 5
TID: 5 - Ticks: 10 - Num: 17
TID: 5 - Ticks: 10 - Num: 18
TID: 6 - Ticks: 23 - Num: 8
TID: 5 - Ticks: 10 - Num: 19
TID: 7 - Ticks: 33 - Num: 6
TID: 5 - Ticks: 10 - Num: 20
TID: 6 - Ticks: 23 - Num: 9
TID: 8 - Ticks: 71 - Num: 3
```
The order of output is order by `Ticks*Num` in ascending order. After each print, the task will wait for another delay time. The product of ticks and num is an indicator of an relative time from the time of creation of the 4 tasks. The program will hang and won't have a exit to RedBoot since there is no way to transfer the information that all the timer clients has died. The CPU usage keeps on increasing after all the tasks has died since the CPU will spend more time being idle than anything else.